import streamlit as st
import pandas as pd
import requests
from io import BytesIO
from openai import OpenAI
from dotenv import load_dotenv
import os
import certifi
import pdfplumber
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from serpapi import GoogleSearch
from generate_pdf_report import generate_pdf_report
import re

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o")

def run_with_spinner(label, func, *args, **kwargs):
    with st.spinner(label):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            st.error(f"{label} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def get_official_site(company_name):
    search = GoogleSearch({
        "q": f"{company_name} å…¬å¼ã‚µã‚¤ãƒˆ",
        "hl": "ja",
        "api_key": os.getenv("SERPAPI_KEY")
    })
    results = search.get_dict()
    try:
        return results["organic_results"][0]["link"]
    except (KeyError, IndexError):
        return ""

def fetch_website_text(url):
    try:
        res = requests.get(url, timeout=5, verify=certifi.where())
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        for script in soup(["script", "style"]):
            script.decompose()
        lines = [line.strip() for line in soup.get_text(separator="\n").splitlines()]
        return "\n".join(line for line in lines if line)
    except Exception as e:
        st.warning(f"{url} å–å¾—å¤±æ•—: {e}")
        return ""

def fetch_all_texts(base_url, max_pages=10):
    visited, to_visit, all_texts = set(), [base_url], []
    domain = urlparse(base_url).netloc
    while to_visit and len(visited) < max_pages:
        current_url = to_visit.pop(0)
        if current_url in visited:
            continue
        visited.add(current_url)
        text = fetch_website_text(current_url)
        if text:
            all_texts.append(text[:5000])
        try:
            res = requests.get(current_url, timeout=5, verify=certifi.where())
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            for a in soup.find_all("a", href=True):
                href = a['href']
                new_url = urljoin(base_url, href)
                if urlparse(new_url).netloc == domain and new_url not in visited and new_url not in to_visit:
                    to_visit.append(new_url)
        except Exception:
            continue
    return "\n\n".join(all_texts)

def extract_text_from_pdf(uploaded_pdf):
    text = ""
    try:
        with pdfplumber.open(uploaded_pdf) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
    except Exception as e:
        st.warning(f"PDFè§£æå¤±æ•—: {e}")
    return text

def generate_personas(company_name, combined_text):
    prompt = f"""
ã‚ãªãŸã¯ãƒˆãƒƒãƒ—æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆå…¼ãƒãƒ¼ã‚±ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€ã“ã®ä¼šç¤¾ã®é¡§å®¢ï¼ˆæ¶ˆè²»è€…ã¾ãŸã¯å–å¼•å…ˆï¼‰ã¨ã—ã¦ã®ãƒšãƒ«ã‚½ãƒŠã‚’4äººä½œæˆã—ã¦ãã ã•ã„ã€‚

é‡è¦:
- {company_name} ã®ç¤¾å“¡ã‚„é–¢ä¿‚è€…ã¯å«ã‚ãªã„ã€‚
- ã‚ãã¾ã§ã“ã®ä¼šç¤¾ã®å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆ©ç”¨ã™ã‚‹é¡§å®¢åƒã§ã™ã€‚

# ä¼šç¤¾å
{company_name}

# Webã‚µã‚¤ãƒˆæƒ…å ± + PDFè³‡æ–™
{combined_text}

## ãƒšãƒ«ã‚½ãƒŠ1ã€œ4
- åå‰
- å¹´é½¢, æ€§åˆ¥, è·æ¥­, æ€§æ ¼, å®¶æ—æ§‹æˆ, è¶£å‘³ãƒ»ä¾¡å€¤è¦³, æ—¥å¸¸ã®æ‚©ã¿,
  ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨ã‚·ãƒ¼ãƒ³, è§£æ±ºã—ãŸã„èª²é¡Œ, ã©ã“ã«æœ€ã‚‚ä¾¡å€¤ã‚’æ„Ÿã˜ã‚‹ã‹,
  æƒ…å ±åé›†ãƒãƒ£ãƒãƒ«, è³¼è²·æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹, ç«¶åˆã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨çŠ¶æ³,
  ä¾¡æ ¼æ„Ÿåº¦, å°å…¥å¾ŒæœŸå¾…ROI, é•·æœŸãƒ“ã‚¸ãƒ§ãƒ³
"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def generate_persona_image(persona_desc):
    prompt = f"""
{persona_desc} ã‚’å‚è€ƒã«ã—ãŸ30-50ä»£ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã®é¡§å®¢ã®æ—¥å¸¸é¢¨æ™¯ã®ã‚¹ãƒŠãƒƒãƒ—ã€‚
è‡ªç„¶å…‰ã€ç”Ÿæ´»æ„Ÿã®ã‚ã‚‹èƒŒæ™¯ã€ãƒªãƒ©ãƒƒã‚¯ã‚¹ã—ãŸè¡¨æƒ…ã€‚
ãƒªã‚¢ãƒ«ãªè³ªæ„Ÿã€æ–‡å­—ã‚„ãƒ©ãƒ™ãƒ«ãªã—ã€åºƒå‘Šã§ã¯ãªããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼é¢¨ã€‚
"""
    img = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size="1024x1024"
    )
    return img.data[0].url

def evaluate_persona_score(persona, idea_name, idea_content):
    prompt = f"""
ã‚ãªãŸã¯ãƒšãƒ«ã‚½ãƒŠæœ¬äººã®æ°—æŒã¡ã«ãªã‚Šãã£ã¦ã€ä¸‹è¨˜ã®äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã€Œè‡ªåˆ†ãªã‚‰æœ¬å½“ã«ä½¿ã„ãŸã„ã‹/ç”Ÿæ´»ãŒã©ã†åºƒãŒã‚‹ã‹ã€ã®è¦³ç‚¹ã§ç¾å®Ÿçš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
100ç‚¹æº€ç‚¹ã§ç‚¹æ•°ã‚’ã¤ã‘ã€ãã®ç†ç”±ã‚’1æ–‡ã§è¿°ã¹ã¦ãã ã•ã„ã€‚
ç‚¹æ•°ã¯åºƒãŒã‚Šã‚’æŒãŸã›ã¦ï¼ˆé«˜å¾—ç‚¹ã‚‚æ™‚ã€…å‡ºã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ï¼‰ã€å¿–åº¦ã›ãšã«æ­£ç›´ã«ã€ãªã‚‹ã¹ãç”Ÿæ´»ã®ä¸­ã§ã®åˆ©ç”¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨åºƒãŒã‚Šã¾ã§æƒ³åƒã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

# ãƒšãƒ«ã‚½ãƒŠ
{persona}

# äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢
åç§°: {idea_name}
å†…å®¹: {idea_content}

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- ã‚¹ã‚³ã‚¢ï¼ˆ100ç‚¹æº€ç‚¹ãƒ»æ•°å­—ã®ã¿ï¼‰
- ç†ç”±ï¼ˆ1æ–‡ã€åˆ©ç”¨ã‚·ãƒ¼ãƒ³ã®æƒ³åƒãƒ»ç”Ÿæ´»ã¸ã®åºƒãŒã‚Šã‚’è¸ã¾ãˆã¦ï¼‰
"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def evaluate_strategy_multiaxis(company_name, idea_name, idea_content):
    prompt = f"""
ã‚ãªãŸã¯æ¥µã‚ã¦ç¾å®Ÿçš„ãªæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¸‹è¨˜ã®äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã€Œå¸‚å ´æ€§ã€ã€Œç«¶äº‰å„ªä½æ€§ã€ã€Œåç›Šæ€§ã€ã€Œå®Ÿç¾å¯èƒ½æ€§ã€ã€Œæˆé•·æ€§ã€ã®5è»¸ã§100ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã€ãã‚Œãã‚Œç†ç”±ã‚’ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ã€‚
è¡¨è¨˜ã‚†ã‚Œãªãå¿…ãšä¸‹è¨˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé€šã‚Šã€ç‚¹æ•°ã¨ç†ç”±ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ä¼šç¤¾å
{company_name}

# äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢
åç§°: {idea_name}
å†…å®¹: {idea_content}

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- å¸‚å ´æ€§: xxç‚¹ï¼ˆç†ç”±: ã€œã€œã€œï¼‰
- ç«¶äº‰å„ªä½æ€§: xxç‚¹ï¼ˆç†ç”±: ã€œã€œã€œï¼‰
- åç›Šæ€§: xxç‚¹ï¼ˆç†ç”±: ã€œã€œã€œï¼‰
- å®Ÿç¾å¯èƒ½æ€§: xxç‚¹ï¼ˆç†ç”±: ã€œã€œã€œï¼‰
- æˆé•·æ€§: xxç‚¹ï¼ˆç†ç”±: ã€œã€œã€œï¼‰
"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def generate_new_potential_personas(company_name, existing_personas, idea_name, idea_content):
    persona_list_str = "\n".join([p[:400] for p in existing_personas])
    prompt = f"""
ã‚ãªãŸã¯ãƒˆãƒƒãƒ—æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¸‹è¨˜ã®äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ã“ã‚Œã¾ã§ã®é¡§å®¢å±¤ã«ã¯ãƒªãƒ¼ãƒã—ã¦ã„ãªã‹ã£ãŸâ€œæ–°ãŸãªæ½œåœ¨é¡§å®¢å±¤â€ã®é–‹æ‹“ã«ã‚‚ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

# äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢
åç§°: {idea_name}
å†…å®¹: {idea_content}

# æ—¢å­˜é¡§å®¢ãƒšãƒ«ã‚½ãƒŠä¸€è¦§
{persona_list_str}

çµ¶å¯¾æ¡ä»¶:
- {company_name}ã®ç¤¾å“¡ã‚„é–¢ä¿‚è€…ã¯å«ã‚ãªã„ã“ã¨
- æ—¢å­˜ãƒšãƒ«ã‚½ãƒŠã¨å±æ€§ã‚„ç‰¹å¾´ãŒé‡è¤‡ã—ãªã„ã€æ–°ãŸãªé¡§å®¢å±¤ï¼ˆæ½œåœ¨ãƒ‹ãƒ¼ã‚ºï¼‰ã‚’2ååˆ†

## ãƒšãƒ«ã‚½ãƒŠå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- åå‰
- å¹´é½¢ãƒ»æ€§åˆ¥ãƒ»è·æ¥­ãƒ»ä¾¡å€¤è¦³ãƒ»æ—¥å¸¸ã®æ‚©ã¿ãƒ»ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã§æƒ¹ã‹ã‚Œã‚‹ç†ç”±
"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def extract_persona_names(persona_list):
    names = []
    for persona in persona_list:
        m = re.search(r'åå‰[:ï¼š]\s*([^\n]+)', persona)
        if m:
            names.append(m.group(1).strip())
        else:
            m2 = re.search(r'[:ï¼š]\s*([^\sï¼ˆ\(\-]+)', persona.split('\n')[0])
            if m2:
                names.append(m2.group(1))
            else:
                names.append(f"ãƒšãƒ«ã‚½ãƒŠ{len(names)+1}")
    return names

st.title("é¡§å®¢ãƒšãƒ«ã‚½ãƒŠÃ—äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢è©•ä¾¡ãƒ„ãƒ¼ãƒ«")

# STEP 1
st.header("STEP 1: ä¼æ¥­æƒ…å ±")
col1, col2 = st.columns(2)
company_name = col1.text_input("â‘  ä¼æ¥­åï¼ˆä¾‹: ä¸‰è¶Šä¼Šå‹¢ä¸¹ï¼‰")
manual_url = col2.text_input("â‘¡ URLã‚’ç›´æ¥æŒ‡å®š")
uploaded_pdf = st.file_uploader("â‘¢ ä¸­æœŸçµŒå–¶è¨ˆç”»ã‚„æ±ºç®—è³‡æ–™ãªã©ã®PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ãƒ»è¤‡æ•°å¯ï¼‰", type=["pdf"], accept_multiple_files=True)
generate_images = st.checkbox("ãƒšãƒ«ã‚½ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã‚‚ç”Ÿæˆã™ã‚‹")

if st.button("ğŸš€ STEP1: ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã‚¹ã‚¿ãƒ¼ãƒˆ"):
    combined_text = ""
    url = get_official_site(company_name) if company_name and not manual_url else manual_url
    if url:
        web_text = run_with_spinner("è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...", fetch_all_texts, url)
        combined_text += web_text + "\n"

    if uploaded_pdf:
        for pdf_file in uploaded_pdf:
            pdf_text = run_with_spinner(f"{pdf_file.name} ã‚’è§£æä¸­...", extract_text_from_pdf, pdf_file)
            combined_text += pdf_text + "\n"

    combined_text = combined_text[:60000]

    if combined_text:
        personas_text = run_with_spinner("ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆä¸­...", generate_personas, company_name or manual_url, combined_text)
        if not personas_text:
            st.error("ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚")
        else:
            persona_list = personas_text.split("## ãƒšãƒ«ã‚½ãƒŠ")[1:]
            persona_images, persona_images_bytes = [], []
            for i, persona in enumerate(persona_list, start=1):
                if generate_images:
                    img_url = run_with_spinner(f"ãƒšãƒ«ã‚½ãƒŠ{i} ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ç”Ÿæˆä¸­...", generate_persona_image, persona)
                    persona_images.append(img_url)
                    try:
                        img_bytes = requests.get(img_url, verify=False).content
                        persona_images_bytes.append(BytesIO(img_bytes))
                    except Exception as e:
                        st.warning(f"ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                        persona_images_bytes.append(None)
                else:
                    persona_images.append(None)
                    persona_images_bytes.append(None)
            st.session_state["persona_list"] = persona_list
            st.session_state["persona_images"] = persona_images
            st.session_state["persona_images_bytes"] = persona_images_bytes
            st.success("âœ… ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆå®Œäº†ï¼")
    else:
        st.warning("ä¼æ¥­æƒ…å ±ï¼ˆWeb / PDFï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

if "persona_list" in st.session_state:
    st.header("ã€ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã€‘")
    persona_names = extract_persona_names(st.session_state["persona_list"])
    for i, (persona, img_url, name) in enumerate(zip(st.session_state["persona_list"], st.session_state["persona_images"], persona_names), start=1):
        cols = st.columns([1,3])
        with cols[0]:
            if img_url:
                st.image(img_url, caption=f"{name}")
        with cols[1]:
            st.text(persona.strip()[:1000] + "...")

# STEP 2
st.header("STEP 2: äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢å…¥åŠ›ï¼ˆè¤‡æ•°å¯ï¼‰")
if "idea_texts" not in st.session_state:
    st.session_state["idea_texts"] = [("", "")]
if "add_idea_flag" not in st.session_state:
    st.session_state.add_idea_flag = False

uploaded_file = st.file_uploader("CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆäº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢å, äº‹æ¥­å†…å®¹ï¼‰", type=["csv"])
ideas = []

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    for _, row in df.iterrows():
        ideas.append((row["äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢å"], row["äº‹æ¥­å†…å®¹"]))
else:
    for i, (name, content) in enumerate(st.session_state["idea_texts"]):
        name = st.text_input(f"äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢{i+1} åç§°", value=name, key=f"idea_name_{i}")
        content = st.text_area(f"äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢{i+1} å†…å®¹", value=content, key=f"idea_content_{i}")
        st.session_state["idea_texts"][i] = (name, content)

    if st.button("äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è¿½åŠ "):
        st.session_state.add_idea_flag = True

    if st.session_state.add_idea_flag:
        st.session_state["idea_texts"].append(("", ""))
        st.session_state.add_idea_flag = False
        st.rerun()

    ideas = [idea for idea in st.session_state["idea_texts"] if idea[0] and idea[1]]

if st.button("ğŸš€ STEP2: å…¨ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è©•ä¾¡"):
    persona_list = st.session_state.get("persona_list", [])
    persona_images_bytes = st.session_state.get("persona_images_bytes", [])
    persona_names = extract_persona_names(persona_list)
    if persona_list and ideas:
        records = []
        strategy_eval_dict = {}
        new_potential_personas_dict = {}
        for idea_name, idea_content in ideas:
            result_row = {"äº‹æ¥­ã‚¢ã‚¤ãƒ‡ã‚¢å": idea_name}
            # å„ãƒšãƒ«ã‚½ãƒŠå—å®¹æ€§è©•ä¾¡
            for i, persona in enumerate(persona_list, start=1):
                eval_result = run_with_spinner(f"{idea_name} Ã— {persona_names[i-1]} å—å®¹æ€§è©•ä¾¡ä¸­...", evaluate_persona_score, "## ãƒšãƒ«ã‚½ãƒŠ"+persona, idea_name, idea_content)
                try:
                    score_line = next(line for line in eval_result.splitlines() if "ã‚¹ã‚³ã‚¢" in line or "ç‚¹" in line)
                    score = re.search(r'(\d+)', score_line).group(1)
                except Exception:
                    score = "N/A"
                try:
                    reason_line = next(line for line in eval_result.splitlines() if "ç†ç”±" in line)
                    reason = reason_line.split(":")[-1].strip()
                except StopIteration:
                    reason = ""
                result_row[f"ãƒšãƒ«ã‚½ãƒŠ{i}ã‚¹ã‚³ã‚¢"] = score
                result_row[f"ãƒšãƒ«ã‚½ãƒŠ{i}ç†ç”±"] = reason

            # å¤šè»¸æˆ¦ç•¥è©•ä¾¡
            multi_eval_result = run_with_spinner(f"{idea_name} å¤šè»¸æˆ¦ç•¥è©•ä¾¡ä¸­...", evaluate_strategy_multiaxis, company_name, idea_name, idea_content)
            strategy_eval_dict[idea_name] = multi_eval_result

            # æ–°è¦æ½œåœ¨é¡§å®¢ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆ
            new_persona_text = run_with_spinner(
                f"{idea_name}å‘ã‘ æ–°è¦æ½œåœ¨é¡§å®¢ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆä¸­...",
                generate_new_potential_personas, company_name, persona_list, idea_name, idea_content
            )
            new_potential_personas_dict[idea_name] = new_persona_text
            records.append(result_row)

        df = pd.DataFrame(records)
        st.session_state["result_df"] = df
        st.session_state["latest_persona_list"] = persona_list
        st.session_state["latest_persona_images_bytes"] = persona_images_bytes
        st.session_state["new_potential_personas"] = new_potential_personas_dict
        st.session_state["strategy_eval_dict"] = strategy_eval_dict
        st.dataframe(df)
        st.header("ã€å¤šè»¸æˆ¦ç•¥è©•ä¾¡ã€‘")
        for idea_name, eval_text in strategy_eval_dict.items():
            st.markdown(f"### â–  {idea_name}")
            st.markdown(f"<pre>{eval_text}</pre>", unsafe_allow_html=True)
        st.header("ã€æ–°è¦æ½œåœ¨é¡§å®¢ãƒšãƒ«ã‚½ãƒŠã€‘")
        for idea_name, new_pers in new_potential_personas_dict.items():
            st.markdown(f"### â–  {idea_name}")
            st.markdown(f"<pre>{new_pers}</pre>", unsafe_allow_html=True)
    else:
        st.warning("ã¾ãšSTEP1ã‚’å®Ÿè¡Œã—ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")

if "result_df" in st.session_state and not st.session_state["result_df"].empty:
    st.header("ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‘")
    if st.button("PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        pdf = generate_pdf_report(
            st.session_state["latest_persona_list"],
            st.session_state["result_df"],
            company_name or "manual",
            st.session_state["latest_persona_images_bytes"],
            st.session_state.get("new_potential_personas", None),
            st.session_state.get("strategy_eval_dict", None),
        )
        st.download_button("ğŸ“„ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=pdf, file_name=f"{company_name or 'manual'}_è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ.pdf", mime="application/pdf")
    if st.button("CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        csv = st.session_state["result_df"].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button("CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆUTF-8 Excelå¯¾å¿œï¼‰", data=csv, file_name=f"{company_name or 'manual'}_è©•ä¾¡çµæœ.csv", mime='text/csv')
else:
    st.info("ã¾ãšå…¨ã‚¢ã‚¤ãƒ‡ã‚¢è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
